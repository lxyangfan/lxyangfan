title: 决策树学习
date: 2017-11-29
tag:
- 机器学习
- 决策树
----

## 介绍

决策树模型可以解决分类问题，升级版的决策树还可解决回归问题（CART）称为分类回归树。

决策树的节点分为2类：中间节点和叶子节点。其中中间节点对应训练数据的特征，叶子节点对应分类的结果。

下面举个例子，邮件分类的规则：

| 特征1              | 特征2        | 分类结果 |
| ------------------ | ------------ | -------- |
| email地址包含18+   | 内容包含澳门 | 赌场邮件 |
| email地址不包含18+ | 内容包含澳门 | 正常邮件 |
| email地址包含18+   | 内容包含最大 | 垃圾垃圾 |
| email地址不包含18+ | 内容包含最大 | 正常邮件 |

根据上述规则得出的决策树：

<img src="/assets/20171129/tree1.png" />

中间节点都是特征，分类结果是叶子节点。

## 决策树学习模型

决策树模型可以与`if-then`的规则方法和条件概率对应起来。因为决策树的内部节点都是特征，从根节点（初始特征）出发遍历到叶子节点（分类）可以看成是经过一系列的`if-then`规则选择之后的结果。

此外，
> 决策树还表示在给定特征的条件下类的条件概率分布。

这句话怎么理解？这一条件概率定义为在对特征空间的一个划分的条件下定义一个分类的概率。说白了，就是从根节点（初始特征）出发遍历到叶子节点（分类）可以看成选择一些特征，然后得到叶子节点就得出了一个分类。

条件概率$P(\bf X| \bf Y)$中，$\bf Y$的取值为所有分类的结果，$\bf X$则是特征空间的一个划分（一些特征的组合）。

决策树学习模型本质，是从训练数据出发构建一棵树形结构，树的中间节点为特征，因此学习的过程就是不断迭代的选择特征$X$一直得到叶子节点（即分类$\bf Y$）。

训练的目标是让损失函数最小，即得出的决策树对训练数据的误差最小，但这又可能会陷入过拟合的误区。因为一颗决策树，深度越大，说明对训练数据训练的越细致，会失去部分泛化的能力。解决方式是剪枝。

如何定义损失函数，如何定义剪枝的标准呢？

## 信息熵

熵的概念对我而言是事物有序无序的一个度量，比如越杂乱无序，熵值是越大，越有序，则熵值越小。

在信息论和概率统计中，熵是对随机变量不确定性的度量。熵越大，随机变量不确定性越大，反之越小。

定义离散随机变量$X$,可能的取值包括$\{x\_1,...,x\_n\}$,概率分布:
$$
P(X = x\_i) = p\_i \text{ , i=1...n } \tag {3.1}
$$
随机变量$X$的熵定义为：
$$
H(X) = - \sum_{i=1}^{n} p_i\log(p_i)  \tag {3.2}
$$
因为$H(X)$与$X$的取值无关，仅仅与分布有关，随机变量$X$的熵又可写为$H(p)=H(X)$。

设随机变量$X,Y$的联合概率分布为$P(X,Y)$，有：
$$
P(X=x\_i,Y=y\_j) = p\_{ij} \tag {3.3}
$$

条件概率熵$H(Y|X)$表示在随机变量$X$确定的条件下随机变量$Y$的不确定性，定义为在随机变量$X$给定的条件下条件概率分布的熵的期望：
$$
H(X|Y) = \sum\_{i=1}^{n} p\_{i}H(X=x\_i|Y) \tag {3.4}
$$

## 信息增益和增益率

当随机变量和联合概率分布都是由极大似然估计得来时，所对应的熵和条件概率熵分别成为经验熵和经验条件熵。

信息增益$g(D,X)$定义为经验熵$H(D)$和特征$X$给定条件下条件熵$H(D|X)$之间的差值，表示在给定特征$X$条件下使得类$Y$不确定性减小的程度：

$$
g(D,X) = H(D) - H(D|X)  \tag {4.1}
$$

决策树算法ID3就是以信息增益作为目标，选取特征作为中间节点。但是有个缺点，就是会对取值较多的特征有所偏好，故C4.5算法定义了增益率作为目标：

$$
R(D,X) = \frac{g(D,X)} {H(D)}  \tag {4.2}
$$







## 决策树生成算法

- 输入：训练数据集合$D$，特征集合$A$，分类结果集合$C=\\{c\_1,...,c\_k\\}$
- 输出：决策树$T$
- 过程：
  - (1) `if (` $D$只有一个分类结果$c\_k$ `) `, 返回叶节点$c\_k$ 
  - (2) `if (` $A=\varnothing $ `) `, 返回数据集合$D$中出现最多的分类$c\_k$作为叶节点
  - (3) 根据`划分标准`从特征集合中找出一个特征$a\_i$作为根节点$t$，并根据该特征可能的取值，将集合$D$划分成若干子集$D\_1,...,D\_n$，并更新特征集合$A=A-\\{a\_i\\}$
  - (4) 对生成的子集$D\_1,...,D\_n$和$A$，按照步骤(1)-(3)递归生成子树
  - (5) 返回决策树
- 说明：
  - 决策树的中间节点是特征
  - 决策树的叶子节点是分类结果$c\_k$
  - `划分标准`包括信息增益、增益率等

## 参考

李航 《统计学习》

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      <!--$表示行内元素，$$表示块状元素 -->
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<!--加载MathJax的最新文件， async表示异步加载进来 -->
<script type="text/javascript" async src="https://cdn.staticfile.org/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>